{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs (Background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recounting Mitchell's defintion on machine learning algorithms.\n",
    "\n",
    "\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\"\n",
    "\n",
    "For LLMS this task, as it name suggests, is in regard to languages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per\n",
    "\n",
    "Loss functions quantify how well our model's predictions align with the actual observed values, providing a measure of the model's performance.\n",
    "\n",
    "## Why We Need Loss Functions\n",
    "\n",
    "1. **Quantification of Error**: Loss functions give us a numerical measure of how far off our predictions are from the true values.\n",
    "\n",
    "2. **Optimization Target**: They provide a clear objective for optimization algorithms to minimize, guiding the learning process.\n",
    "\n",
    "3. **Model Comparison**: Loss functions allow us to compare different models or model configurations objectively.\n",
    "\n",
    "4. **Problem-Specific Evaluation**: Different problems require different evaluation metrics, which loss functions can provide.\n",
    "\n",
    "## Categorization of Errors\n",
    "\n",
    "Errors in machine learning can be categorized in several ways:\n",
    "\n",
    "1. **By Problem Type**:\n",
    "   - Regression errors\n",
    "   - Classification errors\n",
    "\n",
    "2. **By Error Magnitude**:\n",
    "   - Absolute error\n",
    "   - Squared error\n",
    "\n",
    "3. **By Error Direction**:\n",
    "   - Overestimation\n",
    "   - Underestimation\n",
    "\n",
    "4. **By Importance**:\n",
    "   - Weighted errors\n",
    "   - Unweighted errors\n",
    "\n",
    "## Major Loss Functions\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Use Case**: Regression problems\n",
    "\n",
    "**Formula**: $MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**Explanation**: MSE calculates the average squared difference between predicted and actual values. It heavily penalizes large errors due to squaring.\n",
    "\n",
    "**Pros**: \n",
    "- Differentiable\n",
    "- Penalizes larger errors more\n",
    "\n",
    "**Cons**: \n",
    "- Sensitive to outliers\n",
    "- Not robust to label noise\n",
    "\n",
    "### 2. Mean Absolute Error (MAE)\n",
    "\n",
    "**Use Case**: Regression problems\n",
    "\n",
    "**Formula**: $MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "\n",
    "**Explanation**: MAE calculates the average absolute difference between predicted and actual values.\n",
    "\n",
    "**Pros**: \n",
    "- Less sensitive to outliers than MSE\n",
    "- Easier to interpret\n",
    "\n",
    "**Cons**: \n",
    "- Not differentiable at zero\n",
    "- May not converge to a unique solution\n",
    "\n",
    "### 3. Huber Loss\n",
    "\n",
    "**Use Case**: Regression problems, especially with outliers\n",
    "\n",
    "**Formula**: \n",
    "$L_\\delta(y, \\hat{y}) = \\begin{cases}\n",
    "    \\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "    \\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "**Explanation**: Huber loss combines the best properties of MSE and MAE. It's quadratic for small errors and linear for large errors.\n",
    "\n",
    "**Pros**: \n",
    "- Robust to outliers\n",
    "- Differentiable everywhere\n",
    "\n",
    "**Cons**: \n",
    "- Requires tuning of the $\\delta$ parameter\n",
    "\n",
    "### 4. Binary Cross-Entropy\n",
    "\n",
    "**Use Case**: Binary classification problems\n",
    "\n",
    "**Formula**: $BCE = -\\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$\n",
    "\n",
    "**Explanation**: Measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "**Pros**: \n",
    "- Works well for imbalanced datasets\n",
    "- Provides probabilistic output\n",
    "\n",
    "**Cons**: \n",
    "- Can be numerically unstable for predictions near 0 or 1\n",
    "\n",
    "### 5. Categorical Cross-Entropy\n",
    "\n",
    "**Use Case**: Multi-class classification problems\n",
    "\n",
    "**Formula**: $CCE = -\\sum_{i=1}^n \\sum_{j=1}^m y_{ij} \\log(\\hat{y}_{ij})$\n",
    "\n",
    "**Explanation**: Generalizes binary cross-entropy to multiple classes. It measures the dissimilarity between the true distribution and the predicted distribution.\n",
    "\n",
    "**Pros**: \n",
    "- Suitable for multi-class problems\n",
    "- Works well with softmax activation\n",
    "\n",
    "**Cons**: \n",
    "- Can suffer from the vanishing gradient problem\n",
    "\n",
    "### 6. Hinge Loss\n",
    "\n",
    "**Use Case**: Support Vector Machines, margin-based classification\n",
    "\n",
    "**Formula**: $L(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})$\n",
    "\n",
    "**Explanation**: Penalizes predictions that are incorrect or not confident enough. Used in maximum-margin classifiers.\n",
    "\n",
    "**Pros**: \n",
    "- Encourages larger margins between classes\n",
    "- Works well for binary classification\n",
    "\n",
    "**Cons**: \n",
    "- Not probabilistic\n",
    "- Can be sensitive to class imbalance\n",
    "\n",
    "### 7. Focal Loss\n",
    "\n",
    "**Use Case**: Imbalanced classification problems\n",
    "\n",
    "**Formula**: $FL(p_t) = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)$\n",
    "\n",
    "Where $p_t$ is the model's estimated probability for the correct class, $\\alpha_t$ is a balancing factor, and $\\gamma$ is the focusing parameter.\n",
    "\n",
    "**Explanation**: Addresses class imbalance by down-weighting the loss for well-classified examples.\n",
    "\n",
    "**Pros**: \n",
    "- Handles class imbalance effectively\n",
    "- Focuses on hard examples\n",
    "\n",
    "**Cons**: \n",
    "- Requires tuning of additional parameters\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Choosing the right loss function is crucial for effective model training. It depends on the specific problem, the nature of the data, and the desired properties of the model's predictions. Understanding the characteristics of different loss functions allows data scientists and machine learning engineers to make informed decisions in model design and optimization.\n",
    "\n",
    "Would you like me to elaborate on any specific aspect of this lecture note or provide more examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
